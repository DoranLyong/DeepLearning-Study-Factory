# 04.2 – [Recurrent neural networks, vanilla and gated (LSTM)](https://youtu.be/5KSGNomPJTE)


### 1. (tip) [How to summarize papers with Notion](https://youtu.be/5KSGNomPJTE?t=15)


### 2. Why do we need to go to a [higher hidden dimension](https://youtu.be/5KSGNomPJTE?t=307)(= high dimensional space)?

* 만약 은닉에서 차원을 키우지 않고 입력과 마찬가지로 2D →2D →2D →2D →...→2D  형태로 임베딩하면 어떻게 될 까? 
* you should watch [SVD decomposition](https://youtu.be/5KSGNomPJTE?t=520) from Glibert strang and etc. 
  * [Gilbert Strang: Singular Value Decomposition](Gilbert Strang: Singular Value Decomposition)
  * [Singular Value Decomposition (SVD): Mathematical Overview](https://youtu.be/nbBvuuNVfco?t=377)
  * [주성분 분석(PCA)의 기하학적 의미, 공돌이의 수학정리노트](https://youtu.be/YEdscCNsinU?t=480)
  * [선형대수학 83강: 특잇값 분해(SVD)[쑤튜브]](https://youtu.be/gxdJYNIvOI0?t=794)



### 3. (today topic): [Recurrent neural nets](https://youtu.be/5KSGNomPJTE?t=662) - handling sequential data 

* ```Vanilla``` and ```Recurrent``` NN 
  * Combinatorial logic 
  * Sequential logic (★★★) - memory 개념이 나옴 



### 4. [Vector to sequence](https://youtu.be/5KSGNomPJTE?t=972) - vec2seq

* grid 형태의 데이터는 전부 다룰 수 있음 
* (Q) [What could be an application that uses this kind of diagram](https://youtu.be/5KSGNomPJTE?t=1075)? (vec → seq 형태로 모델링 할 수 있는 어플리케이션은?)
  * 





***

### 코드 실습 - ([ref](https://github.com/Atcold/pytorch-Deep-Learning/blob/master/08-seq_classification.ipynb))

* 





***

### Reference 

[1] [NYU-DLSP20, website](https://atcold.github.io/pytorch-Deep-Learning/) / 



















